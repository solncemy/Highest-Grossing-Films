{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASrqnun1hBv6",
        "outputId": "c0e73fe0-8f5e-4aae-c400-583588f4a589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sqlite3\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "BASE_WIKI_URL = \"https://en.wikipedia.org\"\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def fetch_page(url: str):\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "def extract_currency_value(amount_str: str):\n",
        "    if not amount_str:\n",
        "        return None\n",
        "    match = re.search(r'\\$([\\d,.]+)', amount_str)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    numeric_str = match.group(1).replace(',', '')\n",
        "    try:\n",
        "        return float(numeric_str)\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "def parse_release_year(text: str, fallback_cell: Optional[BeautifulSoup]):\n",
        "    year_match = re.search(r'(?:\\((\\d{4})\\)|(\\b(19|20)\\d{2}\\b))', text)\n",
        "    if year_match:\n",
        "        return int(year_match.group(1) or year_match.group(2))\n",
        "\n",
        "    if fallback_cell and fallback_cell.text.strip().isdigit():\n",
        "        return int(fallback_cell.text.strip())\n",
        "    return None\n",
        "\n",
        "def extract_infobox_data(infobox: BeautifulSoup):\n",
        "    data = {'director': 'Unknown', 'country': 'Unknown'}\n",
        "\n",
        "    director_th = infobox.find('th', string=re.compile(r'Direct(ed by|or)'))\n",
        "    if director_th:\n",
        "        director_td = director_th.find_next_sibling('td')\n",
        "        if director_td:\n",
        "            directors = [d.strip() for d in director_td.stripped_strings]\n",
        "            data['director'] = ', '.join(directors)\n",
        "\n",
        "    country_th = infobox.find('th', string=re.compile(r'Countr(y|ies)'))\n",
        "    if country_th:\n",
        "        country_td = country_th.find_next_sibling('td')\n",
        "        if country_td:\n",
        "            country_text = re.sub(r'\\[\\d+\\]', '', country_td.get_text())\n",
        "            countries = re.split(r'[,/]\\s*', country_text.split('\\n')[0])\n",
        "            data['country'] = countries[0].strip()\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_film_details(film_path: str):\n",
        "    details = {'director': 'Unknown', 'country': 'Unknown'}\n",
        "    try:\n",
        "        soup = fetch_page(f\"{BASE_WIKI_URL}{film_path}\")\n",
        "        infobox = soup.find('table', class_='infobox')\n",
        "\n",
        "        infobox_data = extract_infobox_data(infobox)\n",
        "        details.update(infobox_data)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return details\n",
        "\n",
        "def find_data_table(soup: BeautifulSoup):\n",
        "    for table in soup.find_all('table', class_='wikitable'):\n",
        "        headers = [th.get_text(strip=True) for th in table.find_all('th')]\n",
        "        if {'Rank', 'Title', 'Worldwide gross'}.issubset(headers):\n",
        "            return table\n",
        "    return None\n",
        "\n",
        "def save_to_database(films: List[Dict[str, Any]]):\n",
        "    with sqlite3.connect('highest_grossing_films.db') as conn:\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS films (\n",
        "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                title TEXT NOT NULL,\n",
        "                release_year INTEGER,\n",
        "                director TEXT,\n",
        "                box_office REAL,\n",
        "                country TEXT\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        insert_data = [\n",
        "            (f['title'], f['release_year'], f['director'], f['box_office'], f['country'])\n",
        "            for f in films\n",
        "        ]\n",
        "        cursor.executemany('''\n",
        "            INSERT INTO films (title, release_year, director, box_office, country)\n",
        "            VALUES (?, ?, ?, ?, ?)\n",
        "        ''', insert_data)\n",
        "        conn.commit()\n",
        "\n",
        "def main():\n",
        "    main_page = fetch_page(\"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\")\n",
        "    data_table = find_data_table(main_page)\n",
        "\n",
        "    if not data_table:\n",
        "        print(\"Main data table not found\")\n",
        "        return\n",
        "\n",
        "    films = []\n",
        "    count = 1\n",
        "    for row in data_table.find_all('tr')[1:]:\n",
        "        cells = row.find_all(['th', 'td'])\n",
        "        if len(cells) < 5:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            title_cell = cells[2].find('a')\n",
        "            if not title_cell:\n",
        "                continue\n",
        "\n",
        "            film_data = {\n",
        "                'id': count,\n",
        "                'title': title_cell.get_text(strip=True),\n",
        "                'release_year': parse_release_year(\n",
        "                    title_cell.get_text(),\n",
        "                    cells[4] if len(cells) > 4 else None\n",
        "                ),\n",
        "                'box_office': extract_currency_value(cells[3].get_text())\n",
        "            }\n",
        "\n",
        "            details = get_film_details(title_cell.get('href'))\n",
        "            film_data.update(details)\n",
        "\n",
        "            films.append(film_data)\n",
        "\n",
        "        except Exception:\n",
        "            pass\n",
        "        count += 1\n",
        "\n",
        "    save_to_database(films)\n",
        "\n",
        "    with open('films_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(films, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zOo8iUh3v1cR"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}